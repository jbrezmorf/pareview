<ServerManagerConfiguration>
  <ProxyGroup name="sources">
    <SourceProxy name="EclipseReader_1" class="vtkPythonProgrammableFilter" label="Eclipse EGRID and UNRST reader">
      <Documentation
        long_help="Read mesh in EGRID format and time dependent data from UNRST output files."
        short_help="Read mesh in EGRID format and time dependent data from UNRST output files.">
      </Documentation>

      <StringVectorProperty
            name="FileName"
            label="FileName"
            initial_string="FileName"
            animateable="1"
            command="SetParameter"
            default_values=""
            number_of_elements="1"
            panel_visibility="advanced">
            <FileListDomain name="files"/>
            <Documentation>
                This property specifies the file name for the reader.
            </Documentation>
      </StringVectorProperty>
      
      <!-- Set output grid type: vtkMultiblockDataSet -->
      <IntVectorProperty command="SetOutputDataSetType"
                         default_values="13"
                         name="OutputDataSetType"
                         number_of_elements="1"
                         panel_visibility="never">
        <Documentation>The value of this property determines the dataset type
        for the output of the programmable filter.</Documentation>
      </IntVectorProperty>

      <StringVectorProperty
        name="Script"
        command="SetScript"
        number_of_elements="1"
        default_values="import threading&#xA;&#xA;if (not hasattr(self,&quot;info_done_event&quot;)):&#xA;    self.info_done_event=threading.Event()&#xA;&#xA;self.info_done_event.wait(1)    &#xA;# main RequestData script  &#xA;self.code.RequestData(self)"
        panel_visibility="advanced">
        <Hints>
         <Widget type="multi_line"/>
        </Hints>
      <Documentation>This property contains the text of a python program that
      the programmable source runs.</Documentation>
      </StringVectorProperty>

      <StringVectorProperty
        name="InformationScript"
        command="SetInformationScript"
        number_of_elements="1"
        default_values="import threading&#xA;import paraview.simple &#xA;import os&#xA;import sys&#xA;import numpy as np&#xA;from vtk.util import numpy_support&#xA;import vtk&#xA;from contextlib import contextmanager&#xA;from collections import namedtuple&#xA;&#xA;&#xA;&#xA;&#xA;'''&#xA;Class for reading Eclipse files and producing Paraview datasets.&#xA;Attributes:&#xA;  self.egrid_header - directory with type definitions for various eclipse headers&#xA;  self.out - output object of the Programmable Filter, to avoid deep copies we store all&#xA;      auxiliary data directly here&#xA;  &#xA;  self.times - times of the restart file; set by RequestInformation&#xA;  self.file_names - file names named tuple with fields egrid and unrst. Set by SetFileName &#xA;  self.grid - egrid data; set by read_egrid&#xA;  self.restart - array of data for time steps in UNRST file; set by read_restart&#xA;  self.output - VTK output object, set by ExtractFilterOutput&#xA;'''&#xA;class EclipseIO :&#xA;  &#xA;    '''&#xA;    Running guard&#xA;    '''&#xA;    @contextmanager&#xA;    def running_guard(self, persistent_object):&#xA;        #print persistent_object, hasattr(persistent_object,'Running')&#xA;        if not hasattr(persistent_object,'Running'):&#xA;            #print &quot;init false&quot;&#xA;            persistent_object.Running=False&#xA;         &#xA;        try:&#xA;            #print persistent_object, persistent_object.Running&#xA;            if not persistent_object.Running:&#xA;                #print &quot;set true&quot;&#xA;                persistent_object.Running=True&#xA;                yield&#xA;            else:&#xA;                pass&#xA;                #print &quot;skip&quot;&#xA;        finally:&#xA;            #print &quot;set false&quot;&#xA;            persistent_object.Running=False&#xA;  &#xA;    VTK_HEXAHEDRON=12&#xA;&#xA;    '''&#xA;    Set definitions of format headers with proper endian (default is big-endian).&#xA;    Call with endian='&lt;' for choosing little-endian.    &#xA;    '''&#xA;    def __init__(self, endian='&gt;'):&#xA;        '''&#xA;        dtype specification for every header is stored under &#xA;        its keyword in the egrid_header record.&#xA;        '''        &#xA;        self.types={'INTE':endian+'i4',&#xA;                    'DOUB':endian+'f8',&#xA;                    'LOGI':endian+'i4',&#xA;                    'CHAR':'a8',&#xA;                    'REAL':endian+'f',&#xA;                    'MESS':'a1'} # no data ??     &#xA;&#xA;        self.block_spec={} &#xA;             &#xA;        self.block_spec['FILEHEAD']=[int,&#xA;          'version',&#xA;          'release_year',&#xA;          'reserved_00',&#xA;          (4,'backward_compatibility_version'), # smallest version that can read the file&#xA;          'grid_type', # 0 - corner point; 1 - unstructured; 2 - hybrid&#xA;          'dual_porosity_model', # 0 - single porosity; 1 - dual porosity; 2 - dual permeability&#xA;          'original_grid_format',&#xA;          ]&#xA;        &#xA;        self.block_spec['MAPUNITS']=[str,'units']&#xA;        &#xA;        self.block_spec['MAPAXES']=[float,          &#xA;          (0,'y_axis_end',2), # x,y coordinate tuple&#xA;          (0,'origin', 2),&#xA;          (0,'x_axis_end',2)&#xA;          ]&#xA;        &#xA;        self.block_spec['GRIDUNIT']=[str,&#xA;           (0,'units', 2)]&#xA;        &#xA;        # grid header for corner point grids&#xA;        self.block_spec['GRIDHEAD']=[int,&#xA;          'grid_type',&#xA;          (0,'dimensions',3), # (nx,ny,nz)&#xA;          'LGR_idx',&#xA;          (25,'numres'),&#xA;          'nseg',&#xA;          'ntheta',&#xA;          (0,'host_box',6) # (lower i, j, k, upper i, j, k)          &#xA;          ]&#xA;&#xA;        self.block_spec['BOXORIG ']=[int,(0,'origin_xyz',3)]&#xA;               &#xA;        self.block_spec['reservoir']=[int,&#xA;          'lower_k_bound', &#xA;          'upper_k_bound',&#xA;          'incomplete_circle',&#xA;          'isolate_reservoir',&#xA;          'lower_lateral_bound',&#xA;          'upper_lateral_bound',&#xA;          ] &#xA;               &#xA;        self.block_spec['SEQNUM  ']=[int,'file_sequence_number']&#xA;        &#xA;        self.block_spec['INTEHEAD']=[int,&#xA;          'creation_time',&#xA;          (3,'units_type'), # 1-metric, 2-field, 3-lab&#xA;          (9,'dimensions',3),&#xA;          (12,'n_active_cells'),&#xA;          (15,'i_phase'), # [1-oil, water, oil/water, gas, oil/gas. gas/water, oil/water/gas]&#xA;          (17,'n_wells'),                                                &#xA;          (18,'n_max_completitions_per_well'),&#xA;          (20,'n_max_wells_per_group'),&#xA;          (21,'n_max_groups'),&#xA;          (25,'n_data_per_well'), # in IWELL array&#xA;          (28,'n_words_per_well'), # n of 8-char words in ZWELL array&#xA;          (33,'n_data_per_completition'), # in ICON array&#xA;          (37,'n_data_per_group'), # in IGRP array&#xA;          (65,'date',3), # date of the report time&#xA;          (95,'program_id'),&#xA;          (176,'n_max_segmented_wells'),&#xA;          (177,'n_max_segments_per_well'),&#xA;          (179,'n_data_per segment') # in ISEG array&#xA;          ]&#xA;        &#xA;        self.block_spec['LOGIHEAD']=[int,&#xA;          (4,'radial_model_flag_300'), # for eclipse 300&#xA;          (5,'radial_model_flag_100'), # for eclipse 100&#xA;          (15,'dual_porosity_flag'),&#xA;          (31,'coal_bed_methane_flag')&#xA;          ]                                     &#xA;        &#xA;        self.block_spec['DOUBHEAD']=[float,'time_in_days'] # day of the report step&#xA;&#xA;        self.block_spec['well_data']=[int,&#xA;          (0,'wellhead_pos_ijk',3),&#xA;          'n_connections',&#xA;          'i_group',&#xA;          'well_type', #1-producer; 2-oil injection; 3-water injection; 4-gass injection&#xA;          (11,'well_status'), # &gt;0 open; &lt;=0 shut&#xA;          (43,'i_LGR'),&#xA;          (49,'friction_flag'),&#xA;          (71, 'segment_well_number') # =0 for ordinary wells&#xA;          ]&#xA;&#xA;        self.block_spec['completion']=[int,&#xA;          'connection_index', # -IC if no in current LGR&#xA;          (0,'coordinates',3), # ijk ???&#xA;          (6,'status'), # connection status &gt;0 open, &lt;=0 shut&#xA;          (14, 'penetration_direction'), # 1=x, 2=y, 3=z, 4=fractured in x, 5=fractured in y&#xA;          (15, 'segment') # segment containing connection, 0- for multi segment wells&#xA;          ]&#xA;        &#xA;        # Every field is tuple (in_label, out_label, unit)&#xA;        solution_fields=[&#xA;          (&quot;PRESSURE&quot;, &quot;Pressure&quot;,&quot;&quot;),&#xA;          (&quot;SWAT&quot;,      &quot;WaterSat&quot;,&quot;&quot;),&#xA;          (&quot;SGAS&quot;,      &quot;GasSat&quot;,&quot;&quot;),&#xA;          (&quot;SOIL&quot;,      &quot;OilSat&quot;,&quot;&quot;),&#xA;          (&quot;RS&quot;,        &quot;GasOilRatio&quot;,&quot;&quot;),&#xA;          (&quot;RV&quot;,        &quot;OilGasRatio&quot;,&quot;&quot;),&#xA;          (&quot;OWC&quot;,       &quot;OilWaterContact&quot;,&quot;&quot;),&#xA;          (&quot;OGC&quot;,       &quot;OilGasContact&quot;,&quot;&quot;),&#xA;          (&quot;GWC&quot;,       &quot;GasWaterContact&quot;,&quot;&quot;),&#xA;          (&quot;OILAPI&quot;,    &quot;OilAPI&quot;,&quot;&quot;),&#xA;          (&quot;FIPOIL&quot;,    &quot;OilFIP&quot;,&quot;&quot;),&#xA;          (&quot;FIPGAS&quot;,    &quot;GasFIP&quot;,&quot;&quot;),&#xA;          (&quot;FIPWAT&quot;,    &quot;WaterFIP&quot;,&quot;&quot;),&#xA;          (&quot;OIL-POTN&quot;,  &quot;OilPotential&quot;,&quot;&quot;),&#xA;          (&quot;GAS-POTN&quot;,  &quot;GasPotential&quot;,&quot;&quot;),&#xA;          (&quot;WAT-POTN&quot;,  &quot;WaterPotential&quot;,&quot;&quot;),&#xA;          (&quot;POLYMER&quot;,   &quot;PolymerConc&quot;,&quot;&quot;),&#xA;          (&quot;PADS&quot;,      &quot;PolymerAdsorbedConc&quot;,&quot;&quot;),&#xA;          (&quot;XMF&quot;,       &quot;LiquidMoleFrac&quot;,&quot;&quot;),&#xA;          (&quot;YMF&quot;,       &quot;VaporMoleFrac&quot;,&quot;&quot;),&#xA;          (&quot;ZMF&quot;,       &quot;TotalMoleFrac&quot;,&quot;&quot;),&#xA;          (&quot;SSOL&quot;,      &quot;SolventSat&quot;,&quot;&quot;),&#xA;          (&quot;PBUB&quot;,      &quot;BubblePressure&quot;,&quot;&quot;),&#xA;          (&quot;PDEW&quot;,      &quot;DewPressure&quot;,&quot;&quot;),&#xA;          (&quot;SURFACT&quot;,   &quot;SurfaceInteraction&quot;,&quot;&quot;),&#xA;          (&quot;SURFADS&quot;,   &quot;AdsorbedSurfactant&quot;,&quot;&quot;),&#xA;          (&quot;SURFMAX&quot;,   &quot;MaxSurfactantConc&quot;,&quot;&quot;),&#xA;          (&quot;SURFCNM&quot;,   &quot;SurfactantCapilaryNumber&quot;,&quot;&quot;),&#xA;          (&quot;GGI&quot;,       &quot;GI_InjectedGasRatio&quot;,&quot;&quot;),&#xA;          (&quot;WAT-PRES&quot;,  &quot;WaterPressure&quot;,&quot;&quot;),&#xA;          (&quot;WAT_PRES&quot;,  &quot;WaterPressure&quot;,&quot;&quot;),&#xA;          (&quot;GAS-PRES&quot;,  &quot;GasPressure&quot;,&quot;&quot;),&#xA;          (&quot;GAS_PRES&quot;,  &quot;GasPressure&quot;,&quot;&quot;),&#xA;          (&quot;OIL-VISC&quot;,  &quot;OilViscosity&quot;,&quot;&quot;),&#xA;          (&quot;OIL_VISC&quot;,  &quot;OilViscosity&quot;,&quot;&quot;),&#xA;          (&quot;VOIL&quot;,      &quot;OilViscosity&quot;,&quot;&quot;),&#xA;          (&quot;WAT-VISC&quot;,  &quot;WaterViscosity&quot;,&quot;&quot;),&#xA;          (&quot;WAT_VISC&quot;,  &quot;WaterViscosity&quot;,&quot;&quot;),&#xA;          (&quot;VWAT&quot;,      &quot;WaterViscosity&quot;,&quot;&quot;),&#xA;          (&quot;GAS-VISC&quot;,  &quot;GasViscosity&quot;,&quot;&quot;),&#xA;          (&quot;GAS_VISC&quot;,  &quot;GasViscosity&quot;,&quot;&quot;),&#xA;          (&quot;VGAS&quot;,      &quot;GasViscosity&quot;,&quot;&quot;),&#xA;          (&quot;OIL-DEN&quot;,   &quot;OilDensity&quot;,&quot;&quot;),&#xA;          (&quot;OIL_DEN&quot;,   &quot;OilDensity&quot;,&quot;&quot;),&#xA;          (&quot;WAT-DEN&quot;,   &quot;WaterDensity&quot;,&quot;&quot;),&#xA;          (&quot;WAT_DEN&quot;,   &quot;WaterDensity&quot;,&quot;&quot;),&#xA;          (&quot;GAS-DEN&quot;,   &quot;GasDensity&quot;,&quot;&quot;),&#xA;          (&quot;GAS_DEN&quot;,   &quot;GasDensity&quot;,&quot;&quot;),&#xA;          (&quot;DRAINAGE&quot;,  &quot;DrainageRegionNumber&quot;,&quot;&quot;)&#xA;          ]&#xA;        self.solution_fields={}&#xA;        for item in solution_fields:&#xA;            (key,label,unit)=item&#xA;            self.solution_fields[key]=(label,unit)&#xA;    &#xA;    '''&#xA;    Delete whole class as it is after __init__. Namely delete all VTK objects.&#xA;    This is called only when file name has changed.&#xA;    '''&#xA;    def __del__(self):&#xA;        #? should I delete these?&#xA;        #del self.output.corners&#xA;        #del self.output.cells&#xA;        #del self.output.points&#xA;        del self.file_names&#xA;        pass&#xA;&#xA;    '''&#xA;    Get output object from the filter, check its type and store in self.&#xA;    '''&#xA;    def ExtractFilterOutput(self, program_filter):&#xA;            multi_output = program_filter.GetOutput()&#xA;            if not multi_output.IsA(&quot;vtkMultiBlockDataSet&quot;):&#xA;                print &quot;Wrong output data type. Should be vtkMultiBlockDataSet.&quot;&#xA;                raise SystemExit&#xA;            self.output=multi_output        &#xA;        &#xA;        &#xA;    '''&#xA;    Separate filename base and creates filename for egrid file and restart&#xA;    file.&#xA;    '''&#xA;    def SetFileName(self, file_name):&#xA;        (base, ext)=os.path.splitext(file_name)&#xA;        egrid=base+&quot;.egrid&quot;&#xA;        unrst=base+&quot;.unrst&quot;&#xA;        if not os.path.isfile(egrid):&#xA;            egrid=None&#xA;        &#xA;        if not os.path.isfile(unrst):&#xA;            unrst=None&#xA;        FileNames=namedtuple('FileNames', 'egrid unrst')        &#xA;        self.file_names=FileNames(egrid, unrst)&#xA;    &#xA;    &#xA;    '''&#xA;    Search given 'keyword' in the given binary file 'f'.&#xA;    Seeks file 'f' to the first byte of  the keyword. &#xA;    Returns -1 on fail.&#xA;    Note that keywords for eclipse files are always upper case 8 character long.&#xA;    '''&#xA;    @staticmethod&#xA;    def skip_to_keyword(f, keyword):&#xA;        data_str=&quot;&quot;&#xA;        while True:&#xA;            new_data=f.read(1024*4)&#xA;            if (not new_data):&#xA;                return -1&#xA;            data_str += new_data &#xA;            pos=data_str.find(keyword)&#xA;            if (pos &gt;= 0):&#xA;                f.seek(-(len(data_str)-pos),os.SEEK_CUR)&#xA;                return 1&#xA;            else:&#xA;                data_str=data_str[len(data_str) - len(keyword):]&#xA;                &#xA;    &#xA;    '''&#xA;    Check that next block has correct keyword.&#xA;    If yes, read the block to the array of appropriate type and return it.&#xA;    If no, seek back and return None.&#xA;    '''&#xA;    def read_array(self, f, keyword=None, check_type=None, check_size=None):            &#xA;        in_keyword=f.read(8)&#xA;        if (keyword is not None and keyword!=in_keyword):&#xA;            f.seek(-8, os.SEEK_CUR)&#xA;            return None&#xA;          &#xA;&#xA;        size=np.fromfile(f, dtype=np.dtype(self.types['INTE']), count=1 )[0]&#xA;        elem_type=np.fromfile(f, dtype=np.dtype('S4'), count=1 )[0]&#xA;        elem_dtype=np.dtype(self.types[elem_type]) # possibly catch KeyError exception&#xA;        &#xA;        if (check_type is not None): &#xA;            assert(self.types[check_type]==elem_dtype)&#xA;        if (check_size is not None):&#xA;            assert(check_size==size)&#xA;        f.read(8) # 0x10 block_code&#xA;        array=np.fromfile(f, dtype=elem_dtype, count=size)&#xA;        f.read(8) # block_code 0x10&#xA;        &#xA;        if (keyword is None):&#xA;            return (in_keyword, array)&#xA;        else:  &#xA;            return array&#xA;             &#xA;    &#xA;    &#xA;    &#xA;    '''&#xA;    Numpy array wrapper which check validity of indices and slices &#xA;    and return None for an index out of the array.&#xA;    Further provides conversion for a dictionary.&#xA;    '''&#xA;    class Array:&#xA;      &#xA;        '''&#xA;        Construct from a one dim (numpy) array&#xA;        '''&#xA;        def __init__(self, np_array):&#xA;            self.array=np_array&#xA;            &#xA;        def __getitem__(self, key):&#xA;            if (isinstance(key, int)):&#xA;                if (key&gt;len(self.array)): return None&#xA;            elif (isinstance(key,slice)):&#xA;                if (key.stop&gt;len(self.array)): return None&#xA;            return self.array[key]  &#xA;&#xA;        '''&#xA;        Read a block by read_array and convert it to dictionary&#xA;        using specification is self.header[keyword]. One block&#xA;        specification is an array, where the first is singleton of type of &#xA;        produced dictionary items. Elements from input array are processed along with &#xA;        items of specification array assigning the values from the input array to the keys&#xA;        given by the specification array. Specification may contain also tuple (position, key),&#xA;        to give explicit position of the key in the input array. Position must be greater then current &#xA;        position in the input array effectively skip some of its elements.&#xA;        Position is numbered from 1. &#xA;        &#xA;        Examples:&#xA;        'name' - set key 'name' to current value&#xA;        (20,'name') - jump forward to position 20 and set that value to the key 'name'&#xA;        (0,'name') - same as simply 'name'&#xA;        (0,'name',3) - form array from 3 following values and assign to the key 'name'&#xA;        (10,'name',3) - jump forward and form array ...&#xA;        '''&#xA;        def make_dict(self, spec):&#xA;            in_array=self.array&#xA;            if (in_array is None or len(in_array) is 0):&#xA;                return None&#xA;            assert( np.can_cast(type(in_array[0]), spec[0]) &#xA;                   or spec[0] == bool)  &#xA;            i_in_array=0&#xA;            &#xA;            result_dict={}&#xA;            for item in spec[1:]:&#xA;                vector_size=None # default are scalar values&#xA;                if type(item) is tuple:&#xA;                    position=item[0]-1&#xA;                    if (position&gt;0): &#xA;                        assert(position &gt;= i_in_array)&#xA;                        i_in_array=position               &#xA;                    key=item[1]&#xA;                    if len(item)==3:&#xA;                        vector_size=item[2]&#xA;                else:&#xA;                    key=item&#xA;                              &#xA;                              &#xA;                if (i_in_array &gt;= len(in_array)):&#xA;                    new_value=None&#xA;                else:    &#xA;                    if vector_size is None:&#xA;                        # scalar value&#xA;                        new_value=in_array[i_in_array]&#xA;                        i_in_array+=1&#xA;                    else:&#xA;                        # vector value&#xA;                        new_value=in_array[i_in_array:i_in_array+vector_size]&#xA;                        i_in_array+=vector_size                &#xA;                # print &quot;Item: &quot;, key, i_in_array, new_value&#xA;                result_dict[key]=new_value    &#xA;                &#xA;            return result_dict        &#xA;&#xA;    &#xA;    &#xA;    '''&#xA;    Read a block by read_array and convert it to dictionary&#xA;    using specification is self.header[keyword]. ..&#xA;    '''&#xA;    def read_dict(self,f,keyword):&#xA;        spec=self.block_spec[keyword]&#xA;        in_array=self.read_array(f,keyword)&#xA;        return self.Array(in_array).make_dict(spec)        &#xA;              &#xA;    &#xA;    &#xA;    '''&#xA;    Reads EGRID file with name given by parameter 'filename' and&#xA;    store it into (empty) output object given by 'pdo' parameter.&#xA;    Returns updated pdo object.&#xA;    '''&#xA;    def read_egrid(self):&#xA;        if not self.file_names.egrid:&#xA;            print &quot;No grid file. ABORT.&quot;&#xA;            raise SystemExit&#xA;          &#xA;        with open(self.file_names.egrid, 'rb') as f:&#xA;            # skip one int&#xA;            f.read(4)&#xA;            grid={}&#xA;            &#xA;            grid['filehead']=self.read_dict(f, &quot;FILEHEAD&quot;)&#xA;            grid['mapunits']=self.read_dict(f, &quot;MAPUNITS&quot;)        &#xA;            grid['mapaxes']=self.read_dict(f,&quot;MAPAXES&quot;)&#xA;            &#xA;            grid['gridhead']=self.read_dict(f,&quot;GRIDHEAD&quot;)&#xA;            (nx,ny,nz) = grid['dimensions'] = grid['gridhead']['dimensions']&#xA;            &#xA;            nlines=(nx+1)*(ny+1)*grid['gridhead']['numres']&#xA;&#xA;            grid['boxorig']=self.read_dict(f,&quot;BOXORIG &quot;)       &#xA;            grid['lines']=self.read_array(f,&quot;COORD   &quot;, 'REAL', 6*nlines)&#xA;            grid['lines'].shape=(nlines,6)&#xA;            &#xA;            res_data=self.read_array(f,&quot;COORDSYS&quot;, 'INTE', 6*grid['gridhead']['numres'])&#xA;            if (res_data):&#xA;                res_data.shape=(grid['gridhead']['numres'], 6)&#xA;                grid['reservoirs']=res_data&#xA;            &#xA;            grid['z_corners']=self.read_array(f,&quot;ZCORN   &quot;, 'REAL', 8*nx*ny*nz)&#xA;            grid['active_cells']=self.read_array(f,&quot;ACTNUM  &quot;, 'INTE', nx*ny*nz) &#xA;            # 0-inactive, 1-active, 2-active fracture, 3-active matrix and fracture &#xA;            &#xA;            grid['coarsening']=self.read_array(f,&quot;CORSNUM &quot;, 'INTE', nx*ny*nz) &#xA;            grid['hostcells']=self.read_array(f,&quot;HOSTNUM &quot;, 'INTE', nx*ny*nz) &#xA;            self.read_array(f,&quot;ENDGRID &quot;)&#xA;            &#xA;            assert(grid['lines'] != None)&#xA;            assert(grid['z_corners'] != None)&#xA;            &#xA;            #print &quot;LINES\n&quot;,lines&#xA;            #print &quot;CORNERS\n&quot;,z_corners&#xA;            self.grid=grid&#xA;        &#xA;&#xA;&#xA;    '''&#xA;    Create corresponding VTK mesh in given output object, that&#xA;    should be vtkUnstructuredGrid. Takes data from self.grid &#xA;    and assumes that they persist. New data&#xA;    are created in provided object output.&#xA;    '''&#xA;    def create_grid(self, output):&#xA;        if not output.IsA(&quot;vtkUnstructuredGrid&quot;):&#xA;            print &quot;Creating grid in wrong dataset. Should be vtkUnstructuredGrid&quot;&#xA;            raise SystemExit&#xA;        &#xA;        grid=self.grid&#xA;        (nx,ny,nz)=grid['dimensions']&#xA;        lines=grid['lines']&#xA;        z_corners=grid['z_corners']&#xA;        &#xA;        output.Reset()&#xA;        output.corners=np.empty(3*8*nx*ny*nz, dtype=float)&#xA;        output.cells=np.empty(9*nx*ny*nz, dtype=int)&#xA;        &#xA;        i_point=0&#xA;        i_corner=0&#xA;        i_cell=0&#xA;        &#xA;        # eclipse coordinate system:  &#xA;        #           / &#xA;        #  x  &lt;---|/&#xA;        #         |&#xA;        #         v z&#xA;        &#xA;        &#xA;        # local coordinates (x,y,z)&#xA;        hexahedron_local_vtx=[(0,0,0),(1,0,0),(1,1,0),(0,1,0),(0,0,1),(1,0,1),(1,1,1),(0,1,1)]&#xA;        for iz in xrange(nz) :&#xA;            for iy in xrange(ny) :&#xA;                for ix in xrange(nx) :&#xA;                    # print &quot;CELL = &quot;, i_cell&#xA;                    output.cells[i_cell]=8 # number of vertices&#xA;                    i_cell+=1&#xA;                    # set corners of one cell and cell indices to points&#xA;                    &#xA;                    # cell vertical lines are at (ix,ix+1) x ( iy, iy+1) in coords&#xA;                    # cell z coords are at &#xA;                    &#xA;                    for i_vtx in xrange(8):&#xA;                        output.cells[i_cell]=i_point&#xA;                        i_cell+=1&#xA;                        i_point+=1&#xA;                        &#xA;                        loc_x,loc_y,loc_z=hexahedron_local_vtx[i_vtx]&#xA;                        i_zcoord=2*ix+loc_x + 2*nx*(2*iy+loc_y)+ 2*nx*2*ny*(2*iz+loc_z)&#xA;                        z_coord= z_corners[i_zcoord]&#xA;                        line=lines[(ix+loc_x) + (nx+1)*(iy+loc_y)]&#xA;                        top=(line[0], line[1])&#xA;                        z_top=line[2]&#xA;                        bot=(line[3], line[4])&#xA;                        z_bot=line[5]&#xA;                        t=(z_coord-z_bot)/(z_top-z_bot)&#xA;                        (x_coord,y_coord)= top*t + bot*(1-t)&#xA;                        output.corners[i_corner] = x_coord&#xA;                        i_corner+=1&#xA;                        output.corners[i_corner] = y_coord&#xA;                        i_corner+=1&#xA;                        output.corners[i_corner] = z_coord&#xA;                        i_corner+=1&#xA;                        &#xA;                        # print &quot;    vtx: &quot;, i_vtx, x_coord, y_coord, z_coord&#xA;        &#xA;        output.corners.shape=(8*nx*ny*nz, 3)                  &#xA;        output.points=vtk.vtkPoints()&#xA;        output.points.SetData(numpy_support.numpy_to_vtk(output.corners)) # 8*nx*ny*nz (x,y,z)&#xA;        output.SetPoints(output.points)&#xA;        &#xA;        output.cell_array = vtk.vtkCellArray()&#xA;        output.cell_array.SetCells(nx*ny*nz, numpy_support.numpy_to_vtkIdTypeArray(output.cells)) # nx*ny*nz (n,8*i_point)&#xA;        output.SetCells(self.VTK_HEXAHEDRON, output.cell_array) &#xA;        &#xA;          &#xA;        &#xA;    '''&#xA;    Read a restart file as an array of times&#xA;    self.restart[ step1, step2, ...]&#xA;    '''&#xA;    def read_restart(self):&#xA;        if not self.file_names.unrst:&#xA;            return&#xA;        &#xA;        with open(self.file_names.unrst, 'rb') as f:&#xA;            # skip one int&#xA;            f.read(4)&#xA;            self.restart=[]&#xA;            &#xA;            while (1):&#xA;                one_step={}&#xA;                one_step['seq_num']=self.read_dict(f,'SEQNUM  ')&#xA;                &#xA;                one_step['head']=self.read_dict(f,'INTEHEAD')&#xA;                &#xA;                one_step['head'].update(self.read_dict(f,'LOGIHEAD'))&#xA;                #print one_step['head']&#xA;                data=self.read_array(f,'DOUBHEAD') # !! much more complex then described, skip&#xA;                #print data&#xA;                #one_step['head'].update()&#xA;                &#xA;                n_groups=one_step['head']['n_max_groups']&#xA;                group_data_size=one_step['head']['n_data_per_group']&#xA;                n_wells_in_group=one_step['head']['n_max_wells_per_group']&#xA;                groups_data=self.read_array(f,'IGRP    ', 'INTE', n_groups*group_data_size)&#xA;                groups_data.shape=(n_groups, group_data_size)&#xA;                groups=[]&#xA;                for group_data in groups_data:&#xA;                    one_group_data=self.Array(group_data)&#xA;                    group={}&#xA;                    group['childs']=one_group_data[0:n_wells_in_group-1]&#xA;                    group['n_childs']=one_group_data[n_wells_in_group]&#xA;                    group['group_type']=one_group_data[n_wells_in_group+26]&#xA;                    # 0-well_group, 1-node_group (childs are groups), 2- satellite group, 3-slave group&#xA;                    group['group_level']=one_group_data[n_wells_in_group+27]&#xA;                    group['parent_group']=one_group_data[n_wells_in_group+28]&#xA;                    &#xA;                    groups.append(group)&#xA;                one_step['groups']=groups    &#xA;                self.read_array(f,'SGRP    ')&#xA;                self.read_array(f,'XGRP    ')&#xA;                self.read_array(f,'ZGRP    ')&#xA;                &#xA;                &#xA;                '''&#xA;                n_seg_wells=one_step['n_max_segmented_wells']&#xA;                n_seg_per_well=one_step['n_max_segments_per_well']&#xA;                segment_data_size=one_step['n_data_per_segment']&#xA;                segments_data=self.read_array(f,'ISEG    ', np.types['INTE'], n_seg_wells*n_seg_per_well*segment_data_size)&#xA;                segments_data.shape(n_seg_wells,n_seg_per_well,segment_data_size)&#xA;                for well in segments_data:&#xA;                    for segment in well:&#xA;                        outlet_segment_number=segment[1]&#xA;                        branch_for_segment=segment[3]&#xA;                '''&#xA;                &#xA;                n_wells=one_step['head']['n_wells']&#xA;                well_data_size=one_step['head']['n_data_per_well']&#xA;                wells_data=self.read_array(f,'IWEL    ', 'INTE', n_wells*well_data_size)&#xA;                wells_data.shape=(n_wells, well_data_size)&#xA;                wells=[]&#xA;                for well_data in wells_data:&#xA;                    wells.append(self.Array(well_data)&#xA;                                .make_dict(self.block_spec['well_data']))&#xA;                &#xA;                self.read_array(f,'SWEL    ')&#xA;                self.read_array(f,'XWEL    ')&#xA;                &#xA;                n_words_per_well=one_step['head']['n_words_per_well']&#xA;                well_names_data=self.read_array(f,'ZWEL    ', 'CHAR', n_wells*n_words_per_well)  &#xA;                well_names_data.shape=(n_wells, n_words_per_well)&#xA;                for i_well in xrange(n_wells):&#xA;                    name=&quot;&quot;.join(well_names_data[i_well])&#xA;                    wells[i_well]['name']=name.strip()&#xA;                &#xA;                n_completion=one_step['head']['n_max_completitions_per_well']&#xA;                n_per_completion=one_step['head']['n_data_per_completition']&#xA;                completion_data=self.read_array(f,'ICON    ', 'INTE', n_wells*n_completion*n_per_completion)&#xA;                completion_data.shape=(n_wells, n_completion, n_per_completion)&#xA;                for i_well in xrange(n_wells):&#xA;                    completions=[]&#xA;                    for compl in completion_data[i_well]:&#xA;                        completions.append(self.Array(compl)&#xA;                                          .make_dict(self.block_spec['completion']))&#xA;                    wells[i_well]['completions']=completions&#xA;                            &#xA;                one_step['wells']=wells&#xA;                &#xA;                self.read_array(f,'SCON    ')&#xA;                self.read_array(f,'XCON    ')&#xA;                self.read_array(f,'DLYTIM  ')&#xA;                self.read_array(f,'IAAQ    ')&#xA;                self.read_array(f,'SAAQ    ')&#xA;                self.read_array(f,'XAAQ    ')&#xA;                self.read_array(f,'ICAQNUM ')&#xA;                self.read_array(f,'ICAQ    ')&#xA;                self.read_array(f,'SCAQNUM ')&#xA;                self.read_array(f,'SCAQ    ')&#xA;                &#xA;                self.read_array(f,'HIDDEN  ') # skip&#xA;                self.read_array(f,'ZTRACER ') # skip for now&#xA;&#xA;                # read data fields  &#xA;                self.skip_to_keyword(f,'STARTSOL')&#xA;                data1=f.read(24) # 'STARTSOL',0x0,'MESS',0x10,0x10&#xA;                &#xA;                key=&quot;&quot;&#xA;                fields=[]&#xA;                while (1):&#xA;                    (key,array)=self.read_array(f)&#xA;                    if (key == 'ENDSOL  '):&#xA;                        f.seek(-8,os.SEEK_CUR)&#xA;                        break&#xA;                    key=key.strip()&#xA;                    vtk_array=self.make_data_set(key, array)&#xA;                    if vtk_array:&#xA;                        fields.append( vtk_array )      &#xA;                one_step['fields']=fields    &#xA;                self.restart.append(one_step)&#xA;                &#xA;                #buff=str(f.read(4))&#xA;                #print &quot;buff:&quot;, buff&#xA;                #f.seek(-4,os.SEEK_CUR)&#xA;                if (self.skip_to_keyword(f,'SEQNUM  ') == -1):&#xA;                    break &#xA;            # end step loop&#xA;            &#xA;        &#xA;        &#xA;    '''&#xA;    Read only DOUBHEAD sections to retrieve output times.&#xA;    We also check SEQNUM section for consecutive sequence.&#xA;    '''&#xA;    def read_restart_times(self):&#xA;        if not self.file_names.unrst:&#xA;            self.times=[0]&#xA;            return&#xA;          &#xA;        with open(self.file_names.unrst, &quot;rb&quot;) as f:&#xA;            i_time=0&#xA;            times=[]&#xA;            while (1):&#xA;                eof=self.skip_to_keyword(f, 'SEQNUM')&#xA;                if (eof==-1):&#xA;                    break&#xA;                i_time+=1  &#xA;                seq_num=self.read_dict(f,'SEQNUM  ')['file_sequence_number']&#xA;                if (i_time != seq_num):&#xA;                    print &quot;Wrong sequence number&quot;, seq_num, &quot; at position &quot;, i_time&#xA;                    raise AssertionError&#xA;                eof=self.skip_to_keyword(f, 'DOUBHEAD')&#xA;                if (eof==-1):&#xA;                    print &quot;No DOUBHEAD section after SEQNUM section.&quot;&#xA;                    raise AssertionError&#xA;                times.append(self.read_dict(f,'DOUBHEAD')['time_in_days']) &#xA;        self.times=times&#xA;                &#xA;        &#xA;        &#xA;    '''&#xA;    Add new PointData to given vkUnstructuredGrid with given name&#xA;    and data in a numpy array. Assumes a grid &#xA;    '''&#xA;    def make_data_set(self, key, np_array):&#xA;        # check tht key is known&#xA;        if self.solution_fields.has_key(key):&#xA;            name=self.solution_fields[key][0]&#xA;        else:&#xA;            return None&#xA;&#xA;        in_dtype=np_array.dtype&#xA;        if issubclass(in_dtype.type, np.number):&#xA;            if issubclass(in_dtype.type, np.integer):&#xA;                fixed_np_array=np_array.astype(np.dtype('i'))&#xA;                new_array=numpy_support.numpy_to_vtkIdTypeArray(fixed_np_array, deep=True)&#xA;            else:&#xA;                new_array=numpy_support.numpy_to_vtk(np_array.astype(np.dtype('d')), deep=True)&#xA;        else:&#xA;            return None  &#xA;        new_array.SetName(name)&#xA;        return new_array&#xA;&#xA;&#xA;        &#xA;    '''&#xA;    Make datasets from all fields on input.&#xA;    '''&#xA;    def set_all_data_sets(self, one_step, output):&#xA;        for vtk_field in one_step['fields']:&#xA;            name=vtk_field.GetName()&#xA;            cell_data=output.GetCellData()&#xA;            output_array=cell_data.GetArray(name)&#xA;            #help(cell_data.GetArray)&#xA;            if output_array:&#xA;                # this possibly could be done better&#xA;                cell_data.RemoveArray(name)&#xA;                cell_data.AddArray(vtk_field)&#xA;            else:&#xA;                cell_data.AddArray(vtk_field)&#xA;            &#xA;    &#xA;    &#xA;    '''&#xA;    Setting information about the filter output.&#xA;    '''&#xA;    def RequestInformation(self, program_filter):&#xA;        with self.running_guard(program_filter):&#xA;            &#xA;            self.read_restart_times()&#xA;            self.np_times=np.array(self.times)&#xA;            &#xA;            executive=program_filter.GetExecutive()&#xA;            #help(executive.__class__)&#xA;            out_info=executive.GetOutputInformation(0)&#xA;            #out_info=executive.GetOutputInformation().GetInformationObject(0)&#xA;            out_info.Remove(executive.TIME_STEPS())&#xA;            for time in self.times:&#xA;                out_info.Append(executive.TIME_STEPS(), time)&#xA;                #out_info.Append(vtkStreamingDemandDrivePipeline.TIME_STEPS(), time)&#xA;            out_info.Remove(executive.TIME_RANGE())&#xA;            out_info.Append(executive.TIME_RANGE(), self.times[0])&#xA;            out_info.Append(executive.TIME_RANGE(), self.times[-1])&#xA;            #print out_info&#xA;            #print &quot;Times:&quot;, times&#xA;            &#xA;    '''&#xA;    Get timestep to which we should set the data on the grid.&#xA;    '''&#xA;    def GetUpdateTimeStep(self, program_filter):&#xA;        # get requested timestep or the frist one if not present&#xA;        executive = program_filter.GetExecutive()&#xA;        out_info = executive.GetOutputInformation(0)&#xA;        #print out_info&#xA;        if not out_info.Has(executive.UPDATE_TIME_STEP()):&#xA;            return self.times[0]&#xA;        else:&#xA;            return out_info.Get(executive.UPDATE_TIME_STEP())&#xA;&#xA;            &#xA;    '''&#xA;    Setting data to the filter output&#xA;    '''&#xA;    def RequestData(self, program_filter):&#xA;        with self.running_guard(program_filter):&#xA;            self.ExtractFilterOutput(program_filter)&#xA;            timestep=self.GetUpdateTimeStep(program_filter)&#xA;            &#xA;            # optionaly create the grid&#xA;            grid_block=self.output.GetBlock(0)&#xA;            if not grid_block:&#xA;                self.read_egrid()&#xA;                grid_block=vtk.vtkUnstructuredGrid()  &#xA;                self.create_grid(grid_block)&#xA;                self.output.SetBlock(0, grid_block)&#xA;            &#xA;            #if hasattr(self, 'sphere'):&#xA;            #    del self.sphere&#xA;            #self.sphere=vtk.vtkSphereSource()&#xA;            #self.sphere.SetCenter((0,0,timestep))&#xA;            #self.sphere.Update()&#xA;            #self.output.SetBlock(1, self.sphere.GetOutput())                            &#xA;            &#xA;            #optionaly read data&#xA;            if not hasattr(self,&quot;restart&quot;):&#xA;                self.read_restart()&#xA;            &#xA;            # set data&#xA;            &#xA;            #find time&#xA;            i_time=np.abs(self.np_times-timestep).argmin()&#xA;            timestep=self.times[i_time]&#xA;            # make datasets        &#xA;            self.set_all_data_sets(self.restart[i_time], grid_block)&#xA;            # mark correct timestep &#xA;            self.output.GetInformation().Set(self.output.DATA_TIME_STEP(), timestep)&#xA;            &#xA;            # possibly reset camera to get correct view&#xA;            # need working recursion prevention&#xA;            # Finally it is done automaticaly but we may want other orientation of the system.&#xA;            #print &quot;reset camera&quot;&#xA;            #paraview.simple.ResetCamera()&#xA;&#xA;&#xA;&#xA;      &#xA;if (not hasattr(self,&quot;info_done_event&quot;)):&#xA;    self.info_done_event=threading.Event()&#xA;      &#xA;# main RequestInformation script&#xA;# every internal data are stored in self.code&#xA;if hasattr(self, &quot;code&quot;):&#xA;    del self.code&#xA;&#xA;&#xA;FileName=&quot;test.egrid&quot;    &#xA;self.code=EclipseIO()   &#xA;self.code.SetFileName(FileName)&#xA;self.code.RequestInformation(self)&#xA;# indicator that RequestInformation is done&#xA;self.info_done_event.set()&#xA;&#xA; "
        panel_visibility="advanced">
        <Hints>
         <Widget type="multi_line"/>
        </Hints>
      <Documentation>This property contains the text of a python program that
      the programmable source runs.</Documentation>
      </StringVectorProperty>
      
      <DoubleVectorProperty
        name="TimestepValues"
        information_only="1">
        <TimeStepsInformationHelper/>
        <Documentation>
          Available timestep values.
        </Documentation>  
      </DoubleVectorProperty>
      
      <Hints>
        <ReaderFactory extensions="egrid unrst"
                       file_description="Eclipse mesh File Format" />
      </Hints>
    </SourceProxy>
  </ProxyGroup>
</ServerManagerConfiguration>
